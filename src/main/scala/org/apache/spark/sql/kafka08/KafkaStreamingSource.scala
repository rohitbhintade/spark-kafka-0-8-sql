/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.kafka08

import java.io._
import java.nio.charset.StandardCharsets

import scala.annotation.tailrec
import kafka.common.TopicAndPartition
import kafka.message.MessageAndMetadata
import kafka.serializer.DefaultDecoder
import org.apache.spark.SparkException
import org.apache.spark.sql._
import org.apache.spark.sql.execution.streaming._
import org.apache.spark.sql.types._
import org.apache.spark.streaming.kafka.{Broker, KafkaCluster, KafkaUtils, OffsetRange}
import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset
import org.apache.commons.io.IOUtils
import org.apache.commons.lang.StringUtils
import org.apache.hadoop.fs.{Path, UnsupportedFileSystemException}
import org.apache.spark.sql.execution.streaming.HDFSMetadataLog.{FileContextManager, FileManager, FileSystemManager}
import org.apache.spark.sql.kafka08.util.Logging

/**
 * A [[Source]] that uses Kafka's SimpleConsumer API to reads data from Kafka.
 */
private[kafka08] class KafkaStreamingSource(
    sqlContext: SQLContext,
    topics: Set[String],
    kafkaParams: Map[String, String],
    sourceOptions: Map[String, String],
    metadataPath: String,
    startFromSmallestOffset: Boolean)
  extends Source with Logging {

  private var currentOffsets : Map[TopicAndPartition, Long] = Map[TopicAndPartition, Long]()

  private val sc = sqlContext.sparkContext
  private val kc = new KafkaCluster(kafkaParams)
  private val topicPartitions = KafkaCluster.checkErrors(kc.getPartitions(topics))
  override def schema: StructType = KafkaStreamingSource.kafkaSchema
  val baseCheckPointDirPath: String = metadataPath.substring(0,metadataPath.indexOf("/sources/"))
  val fileManager: FileManager = createFileManager()

  val metadataLog: HDFSMetadataLog[KafkaSourceOffset] =
    new HDFSMetadataLog[KafkaSourceOffset](sqlContext.sparkSession, metadataPath) {
    override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {
      out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)
      val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))
      writer.write("v" + KafkaStreamingSource.VERSION + "\n")
      writer.write(metadata.json())
      writer.flush()
    }

    override def deserialize(in: InputStream): KafkaSourceOffset = {
      in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)
      val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))
      // HDFSMetadataLog guarantees that it never creates a partial file.
      assert(content.length != 0)
      if (content(0) == 'v') {
        val indexOfNewLine = content.indexOf("\n")
        if (indexOfNewLine > 0) {
          val version = parseVersion(content.substring(0, indexOfNewLine), KafkaStreamingSource.VERSION)
          KafkaSourceOffset(SerializedOffset(content.substring(indexOfNewLine + 1)))
        } else {
          throw new IllegalStateException(
            s"Log file was malformed: failed to detect the log file version line.")
        }
      } else {
        // The log was generated by Spark 2.1.0
        KafkaSourceOffset(SerializedOffset(content))
      }
    }
  }

  private val maxOffsetFetchRetries = sourceOptions.getOrElse("fetchOffset.numRetries", "3").toInt

  private lazy val initialPartitionOffsets = {
    metadataLog.get(0).getOrElse {
      val offsets = for {
        leaderOffsets <- (if (startFromSmallestOffset) {
          kc.getEarliestLeaderOffsets(topicPartitions)
        } else {
          kc.getLatestLeaderOffsets(topicPartitions)
        }).right
      } yield leaderOffsets

      val kafkaSourceOffset = KafkaSourceOffset(KafkaCluster.checkErrors(offsets))

      metadataLog.add(0, kafkaSourceOffset)
      info(s"Initial offsets: $kafkaSourceOffset")
      kafkaSourceOffset
    }.partitionToOffsets
  }

  def getKafkaSourceOffsetFromCommitLogs(batchId : Long ): KafkaSourceOffset = {

    val path = new Path(baseCheckPointDirPath + "/offsets/" + s"$batchId")
    val in = fileManager.open(path)

    //in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)
    val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))
    // HDFSMetadataLog guarantees that it never creates a partial file.
    assert(content.length != 0)
    if (content(0) == 'v') {
      val indexOfNewLine =  StringUtils.ordinalIndexOf(content, "\n", 2)
      if (indexOfNewLine > 0) {
        KafkaSourceOffset(SerializedOffset(content.substring(indexOfNewLine + 1)))
      } else {
        throw new IllegalStateException(
          s"Log file was malformed: failed to detect the log file version line.")
      }
    } else {
      // The log was generated by Spark 2.1.0
      KafkaSourceOffset(SerializedOffset(content))
    }
  }

  private def createFileManager(): FileManager = {
    val hadoopConf = sqlContext.sparkSession.sessionState.newHadoopConf()
    try {
      new FileContextManager(new Path(baseCheckPointDirPath), hadoopConf)
    } catch {
      case e: UnsupportedFileSystemException =>
        warn("Could not use FileContext API for managing metadata log files at path " +
          s"$metadataPath. Using FileSystem API instead for managing log files. The log may be " +
          s"inconsistent under failures.")
        new FileSystemManager(new Path(baseCheckPointDirPath), hadoopConf)
    }
  }

  val offsetLog = new OffsetSeqLog(sqlContext.sparkSession, checkpointFile("offsets"))

  val batchCommitLog = new BatchCommitLog(sqlContext.sparkSession, checkpointFile("commits"))

  def checkpointFile(name: String): String =
    new Path(new Path(baseCheckPointDirPath), name).toUri.toString

  private def populateStartOffsets(): KafkaSourceOffset = {

    offsetLog.getLatest() match {
      case Some((latestBatchId, nextOffsets)) =>
        /* identify the current batch id: if commit log indicates we successfully processed the
         * latest batch id in the offset log, then we can safely move to the next batch
         * i.e., committedBatchId + 1 */
        batchCommitLog.getLatest() match {
          case Some((latestCommittedBatchId, _)) =>
            if (latestBatchId == latestCommittedBatchId) {
              /* The last batch was successfully committed, so we can safely process a
               * new next batch but first:
               * Make a call to getBatch using the offsets from previous batch.
               * because certain sources (e.g., KafkaSource) assume on restart the last
               * batch will be executed before getOffset is called again. */

              getKafkaSourceOffsetFromCommitLogs(latestCommittedBatchId)

            } else if (latestCommittedBatchId == latestBatchId - 1) {
              getKafkaSourceOffsetFromCommitLogs(latestCommittedBatchId + 1)
            }
            else {
              warn(s"Batch completion log latest batch id is $latestCommittedBatchId," +
                s" which is not trailing batchid $latestBatchId by one")
              getKafkaSourceOffsetFromCommitLogs(latestCommittedBatchId + 1)
            }
          case None => info("no commit log present")
            metadataLog.get(0).get
        }
      case None => // We are starting this stream for the first time.
        info(s"Starting new streaming query.")
        metadataLog.get(0).get
    }
  }


  /** Returns the maximum available offset for this source. */
  override def getOffset: Option[Offset] = {

    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    if (maxRateLimitPerPartition != 0) {
      info("fetched" + populateStartOffsets().partitionToOffsets)
      currentOffsets = populateStartOffsets().partitionToOffsets.map(a => a._1 -> a._2.offset)
      val clampedOffeset = clamp(KafkaSourceOffset(fetchLatestOffsets(maxOffsetFetchRetries)).partitionToOffsets)
      info("clamped" + clampedOffeset)
      Some(KafkaSourceOffset(clampedOffeset))
    }
    else {
      val offset = KafkaSourceOffset(fetchLatestOffsets(maxOffsetFetchRetries))
      info(s"GetOffset: ${offset.partitionToOffsets.toSeq.map(_.toString).sorted}")
      Some(offset)
    }
  }


  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {
    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    info(s"GetBatch called with start = $start, end = $end")
    val untilPartitionOffsets = KafkaSourceOffset.getPartitionOffsets(end)
    val fromPartitionOffsets = start match {
      case Some(prevBatchEndOffset) =>
        KafkaSourceOffset.getPartitionOffsets(prevBatchEndOffset)
      case None =>
        initialPartitionOffsets
    }

    val offsetRanges = fromPartitionOffsets.map { case (tp, fo) =>
      val uo = untilPartitionOffsets(tp)
      OffsetRange(tp.topic, tp.partition, fo.offset, uo.offset)
    }.toArray

    val leaders = untilPartitionOffsets.map { case (tp, lo) =>
      tp -> Broker(lo.host, lo.port)
    }

    val messageHandler = (mmd: MessageAndMetadata[Array[Byte], Array[Byte]]) => {
      Row(mmd.key(), mmd.message(), mmd.topic, mmd.partition, mmd.offset)
    }

    // Create a RDD that reads from Kafka and get the (key, value) pair as byte arrays.
    val rdd = KafkaUtils.createRDD[
      Array[Byte],
      Array[Byte],
      DefaultDecoder,
      DefaultDecoder,
      Row](sc, kafkaParams, offsetRanges, leaders, messageHandler)

    debug("GetBatch generating RDD of offset range: " + offsetRanges.sortBy(_.topic).mkString(","))
    sqlContext.createDataFrame(rdd, schema)
  }

  /** Stop this source and free any resources it has allocated. */
  override def stop(): Unit = { }

  override def toString(): String = s"KafkaSource for topics [${topics.mkString(",")}]"

  private val maxRateLimitPerPartition: Long = sqlContext.sparkContext.getConf.getLong(
    "spark.streaming.kafka.maxRatePerPartition", 0)

  def maxMessagesPerPartition(offsets: Map[TopicAndPartition, Long]): Option[Map[TopicAndPartition, Long]] = {

    val effectiveRateLimitPerPartition = offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition}
    Option(effectiveRateLimitPerPartition)
  }

  // limits the maximum number of messages per partition
  protected def clamp(leaderOffsets: Map[TopicAndPartition, LeaderOffset]): Map[TopicAndPartition, LeaderOffset] = {
    val offsets = leaderOffsets.mapValues(lo => lo.offset)

    maxMessagesPerPartition(offsets).map { mmp =>
      mmp.map { case (tp, messages) =>
        val lo = leaderOffsets(tp)
        tp -> lo.copy(offset = Math.min(currentOffsets(tp) + messages, lo.offset))
      }
    }.getOrElse(leaderOffsets)
  }

  @tailrec
  private def fetchLatestOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {
    val offsets = kc.getLatestLeaderOffsets(topicPartitions)
    if (offsets.isLeft) {
      val err = offsets.left.get.toString
      if (retries <= 0) {
        throw new SparkException(err)
      } else {
        error(err)
        Thread.sleep(kc.config.refreshLeaderBackoffMs)
        fetchLatestOffsets(retries - 1)
      }
    } else {
      offsets.right.get
    }
  }
}

/** Companion object for the [[KafkaStreamingSource]]. */
private[kafka08] object KafkaStreamingSource {

  private[kafka08] val VERSION = 2

  def kafkaSchema: StructType = StructType(Seq(
    StructField("key", BinaryType),
    StructField("value", BinaryType),
    StructField("topic", StringType),
    StructField("partition", IntegerType),
    StructField("offset", LongType)
  ))
}

